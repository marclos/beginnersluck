\documentclass{tufte-handout}

%\geometry{showframe}% for debugging purposes -- displays the margins
\usepackage{verbatim}
\usepackage{amsmath}
%\usepackage{natbib}
%\bibfont{\small} % Doesn't see to work...

% Set up the images/graphics package
\usepackage{graphicx}
%\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% \graphicspath{{graphics/}}


\title{Confidence Intervals %\thanks{}
}
\author[Marc Los Huertos]{Marc Los Huertos}
%\date{}  % if the \date{} command is left out, the current date will be used


% \SweaveOpts{prefix.string=graphics/plot} % Created a "graphics" subdirectory to 

\setsidenotefont{\color{blue}}
% \setcaptionfont{hfont commandsi}
% \setmarginnotefont{\color{blue}}
% \setcitationfont{\color{gray}}

% The following package makes prettier tables.  We're all about the bling!
%\usepackage{booktabs}

% Small sections of multiple columns
%\usepackage{multicol}

\begin{document}

\maketitle% this prints the handout title, author, and date

\begin{abstract}
\noindent An important considerations with univariate data is the development of confidence intervals and forms the foundation for hypothesis testing. As you can probably guess, confidence intervals are based on the distribution of the data. Data with lots of variability will have wide confidence intervals, while data with low variability will have narrow confidence intervals. 

There are two main approaches in developing confidence intervals. One relies on the use of theoretical probability distributions. Another method uses the data themselves to create randomized sample subsets (bootstrapping) to create confidence intervals. We will learn several methods because each are used in environmental sciences to varying degrees. I have decribed the former in this handout, but may add bootstrapping soon. 
\end{abstract}

%\printclassoptions

% Setting up the margins, etc for R
<<echo = F, results = 'hide'>>=
options(width=60)
rm(list = ls())
@

\section{Why Confidence Intervals?}

Confidence intervals are used to indicate the reliability of an estimate. How likely the interval is to contain the parameter is determined by the confidence level. Increasing the desired confidence level will widen the confidence interval (see Figure~\ref{fig:3rule}). Based on a foundational understanding of confidence intervals allows us to extend our interpretations of the data toward a hypothesis testing perspective, which we'll get into more during the semester.

\section{Where are Confidence Intervals Used?}

Confidence intervals can be calculated for a range of statistics, including the mean, slope and intercept of a linear model, among other things. We'll concentrate on the what confidence intervals of the population and sample mean at this point, but keep in mind the concept can be applied to many parameter estimates.

\begin{marginfigure}
	\centering
		\includegraphics[width=1.00\textwidth]{Investor_confidence500.jpg}
	\caption{Confidence abounds without bounds.}
	\label{fig:Investor_confidence500}
\end{marginfigure}

\section{Defining Some Terms}

\subsection{Populations and Samples}

In general, environmental scientists can't measure the entire population. For example, a complete audit of all the recycling would be impossible for our class!  Instead, we sample from the population. Statisticians have have developed semi-consistant symbology for various statistics based on populations and samples. 

A population mean for example is usually referred to by the Greek letter, $\mu$. While for a sample, the mean might be referred to as $\bar{y}$ (or $\bar{x}$). The spread of data, or variance, in a population is referred to as $\sigma^2$, again a Greek letter. The population variance is often referred to as $s^2$. In general, we don't know $\mu$ or $\sigma^2$, so we can estimate them and develop confidence intervals that probably include the true $\mu$. Notice the word, ''probably.''  In other words, we our intervals in terms of probabilities!\sidenote{When you hear the word probably, it might be useful to think about how this implies a probability distribution.}

\subsection{The difference between $\mu$ and $\bar{x}$}

To illustrate the difference, let's create a dataset of 1000 random numbers from a normal distribution with a mean of 10 and standard deviation of 1. We'll let this represent an entire population.\sidenote{I rounded the values to the nearest tenth, to keep the printouts more managable.} 
<<>>=
set.seed(123)
N1000 = round(rnorm(n= 1000, mean = 10, sd = 1), 1)
@

The distribution of the data (Figure~\ref{fig:hist_pop}), resembles a normal distribution, which is reassuring since we created it using a random number generator that uses the normal probability distribution. 

\begin{marginfigure}
<<echo=FALSE, fig=TRUE>>=
par(cex=1.7, las=1)
hist(N1000, main=NA, xlab="Values", xlim=c(5,15), ylim=c(0,250), breaks=20, col='gray')
#box()
@
\caption{Frequency distribution of population.}
\label{fig:hist_pop}
\end{marginfigure}

These data will represent the population (N) and has a mean or $\mu$ of \Sexpr{round(mean(N1000), 2)}. The values range from \Sexpr{range(N1000)[1]} to \Sexpr{range(N1000)[2]} (Figure~\ref{fig:hist_pop}). From this dataset, we can create several dataset sets by random sampling the vector with different n (n = 100, n = 20, n = 10, n = 5). 

<<>>=
n100 = sample(N1000, size = 100)
n20 = sample(N1000, size = 20)
n10 = sample(N1000, size = 10)
n5 = sample(N1000, size = 5)
@

With a size or n of ten values from our population, I have obtained the following result: 
<<>>=
n10
@ 
\noindent and a mean or $\bar{y}$ = \Sexpr{round(mean(n10),2)}. These values certainly fall within the distribution of the population (Figure~\ref{fig:hist_pop}).

\subsection{R's Default Boxplot}

One of the confusing aspects when learning R and statistics at the same time is the range of subtle discrepencies between the vocabulary in statistics, parameter symbology, and how R implments statistics. But just as we learn a language we use content (e.g. literature), I have found it best to learn statistics while learning to use the R language. 

Nevertheless, the sublties land hard on us when we are just trying to keep up with the statistical concepts. The \texttt{boxplot} function in R is easy to make and provides a lot of information, but it's not always clear what is being displayed. For example, the \texttt{boxplot()} function displays the median (for our ten sample vector = \Sexpr{median(n10)} and not the mean (\Sexpr{mean(n10)}). In Figure~\ref{fig:default_bp}A, the boxplot displays the median, interquarile range, and range. But in many cases, the sample might have some points that seem to be way outside the normal range. In those cases R creates a figure with a more complex set of rules. We won't go into these at this point, but not the differences between Figure~\ref{fig:default_bp}A and Figure~\ref{fig:default_bp}B. 

\begin{figure}
\caption{Boxplot where panel A displays the following information for our 10 sample vector: median =\Sexpr{round(median(n10),2)}; Range = \Sexpr{range(n10)}; and interquartile range = \Sexpr{quantile(n10)[c(2,4)]}. Panel B identifies an ``outlier" using obscure rules and changing the range to exclude the outlier. I included the value of 14.8 with the vector to demonstrate this change. Weird. Note: I rotated the y-axis labels to make it easier to read. I try to do this with all my figures to make it easier to read.}
\label{fig:default_bp}
<<fig=T, echo=F, height=3>>=
par(las=1, cex=2, mfrow=c(1,2))
boxplot(n10, col="yellow", ylim=c(9,15))
mtext("A", 3, line=1, at=.5, cex=3)
boxplot(c(n10, 14.8), col="purple", ylim=c(9,15))
mtext("B", 3, line=1, at=.5, cex=3)
@
\end{figure}

%Let's see if we can label the graphic directly!
In the end, the \texttt{boxplot()} function is very useful to quickly summarize the data, but in most cases, we tend to describe the data using a more precise set of measures. The standard deviation is the most commonly used parameter to measure the variability or spread of the data. 

\subsection{Exploring the Waste Audit Data}

Let's `read' the data into r, with the \texttt{read.csv()} function.
<<tidy=TRUE>>=
Unsorted.csv = "/home/CAMPUS/mwl04747/github/beginnersluck/Confidence_Intervals/2019_EA30F_Waste_Audit_Unsorted.csv"

Sorted.csv = "/home/CAMPUS/mwl04747/github/beginnersluck/Confidence_Intervals/2019_EA30F_Waste_Audit_Sorted_v2.csv"
# Read Raw Data
Unsorted = read.csv(Unsorted.csv)
Sorted = read.csv(Sorted.csv)
@

Now that we have imported the data into R, we will not process some of the data to prepare for the analysis. For example, let's caculate the percentage of sorted items, remove the plastic film category, and shorten the compostable name to simply compost. 

<<>>=
Sorted$Percent = (Sorted$NetMass/Sorted$Total)*100
Sorted = subset(Sorted, subset=Type!="Plastic Film")
levels(Sorted$Type)[levels(Sorted$Type)=="Compostable"] <- "Compost"
@

Be sure to check the results as you go and convince yourself how these work by looking online to see how these functions work. 

Anyone who has worked with quantitiative data knows that data entry errors can be a major headache if they are not caught early. Thus, we'll use a couple of methods to evaluate potential data entry errors. 

\begin{figure*}
\caption{Sorted Percent Mass from Recycling Bags}
\label{fig:percent_bp}
<<fig=T, echo=F, fig.height=5>>=
par(mfrow=c(1,2), las=1, cex.axis=1, mar = c(4, 2, 3, 1) + 0.1)
boxplot(Percent ~ Type, data=droplevels(Sorted[Sorted$Trash_Recycle=="R",]), xaxt='none', col="darkgreen")
axis(side=1, at=1:7, label=levels(droplevels(Sorted$Type[Sorted$Trash_Recycle=="R"])), 
    cex.axis=.4)
boxplot(Percent ~ Type, data=droplevels(Sorted[Sorted$Trash_Recycle=="T",]), xaxt='none', col="red")
axis(side=1, at=1:7, label=levels(droplevels(Sorted$Type[Sorted$Trash_Recycle=="T"])), 
    cex.axis=.45)

@
\end{figure*}

In the case of Figure~\ref{fig:percent_bp}, We can see that the trash is relatively high in both types of unsorted bags, e.g. there a fair amount trash in both the recyling and trash containers. But I wonder if the overlapping interval is statistically significant?  If they are the same, then the percentage of trash is being put in both containers without regard of the intended use. The boxplot is useful to start with, but we can't draw statistical conclusions on this yet.  

\subsection{Measuring the Spread}

We can calculate the variance of the sample using the following formula: 
\begin{equation}
s^2 = \frac{\sum(y_i - \bar{y})^2}{n-1}
\end{equation}

Let's apply this equation, so you can see how it works! First, we'll take the difference between a measurment and the mean.\sidenote{We will use the vector index to subset each variable -- see ``Blue Book'' for indexing if you want more information on this procedure.} Let's start with the first observation:
<<>>=
n10[1] - mean(n10)
@

Next we square the difference\sidenote{R did scientific notation on this, I'll need to fix this in the near future.}
<<>>=
(n10[1] - mean(n10))^2
@
\noindent Now we'll do this for each of the remaining observations:
<<>>=
(n10[2] - mean(n10))^2
(n10[3] - mean(n10))^2
(n10[4] - mean(n10))^2
(n10[4] - mean(n10))^2
@
These are ``deviates.'' Then we add the deviates together (which is what the $\Sigma$ symbol is tell us to do). We'll call the sum of squared deviates:

<<>>=
(sum_of_squared_deviates = (n10[1] - mean(n10))^2 + 
   (n10[2] - mean(n10))^2 + (n10[3] - mean(n10))^2 + 
   (n10[4] - mean(n10))^2 + (n10[5] - mean(n10))^2 +
   (n10[6] - mean(n10))^2 + (n10[7] - mean(n10))^2 + 
   (n10[8] - mean(n10))^2 + (n10[9] - mean(n10))^2 + 
   (n10[10] - mean(n10))^2)
@

We take the sum of these and divide by the degrees of freedom.\sidenote{Degree of freedom is extremely complicated to explain without tons of theory. For now, just rest assured that it works. If we have time, we'll delve into this more.}

<<>>=
sum_of_squared_deviates/(10-1)
sqrt(sum_of_squared_deviates/(10-1))
@
\noindent Thankfully, we don't need to use all of this tedious code to generate the variance and standard deviation. We can use \texttt{var()} and \texttt{sd()}. 
<<>>=
var(n10)
sd(n10)
@

\noindent This value is the sample variance. 
The $s^2$ for our sample is \Sexpr{round(var(n10), 2)}. The standard deviation is the $\sqrt(s^2)$ or just $s$. For our sample, the standard deviation is \Sexpr{round(sd(n10), 2)}. 

Then, we can calculate the standard error of the sample using the following equation: 

\begin{equation}
\textrm{Standard Error of the Sample} = SE_s = \sigma/\sqrt{n}
\end{equation}


<<>>=
#computation of the standard error of the sample mean
sem<-sd(n10)/sqrt(length(n10))
@

NOTE: If the sample size approaches the size of the population, we must use a "finite-population correction". Apparently the Central Limit Theorum gets wonky and the correction factor ensures better estimates. We don't need to worry about in in our case.

Now, it's important to note that we have calculated sample estimates. We can also calculate the population parameters as well.

\begin{table}
\begin{tabular}{lllr}
Parameter    &  Symbol  & Formula   & Population value \\ \hline\hline
Mean        & $\mu$     & $\Sigma x_i/n$ & \Sexpr{round(mean(N1000), 2)} \\
Variance        & $\sigma^2$     & $\Sigma(x_i-\mu)^2/n$ & \Sexpr{round(var(N1000), 2)} \\
Standard Deviation        & $\sigma$     & $\sqrt(\Sigma(x_i-\mu)^2/n)$ & \Sexpr{round(sd(N1000), 2)} \\
\hline
\end{tabular}
\end{table}

\subsection{The Standard Deviation versus Standard Error}

The terms ``standard error'' and ``standard deviation'' are often confused. The contrast between these two terms reflects the important distinction between data description and inference, we should appreciate.

The standard deviation (often $s$) is a measure of variability. When we calculate the standard deviation of a sample (which we did above), we are using it as an \textbf{estimate of the variability of the population} from which the sample was drawn. For normally distributed data the standard deviation has some extra information, namely the 68-95-99.7 rule which tells us the percentage of data lying within 1, 2 or 3 standard deviation from the mean. For now, try to remember about 95\% of individuals will have values within 2 standard deviations of the mean, the other \% being equally scattered above and below these limits. 

Contrary to popular misconception, the standard deviation is a valid measure of variability regardless of the distribution. About 95\% of observations of any distribution usually fall within the 2 standard deviation limits, though those outside may all be at one end. We may choose a different summary statistic, however, when data have a skewed distribution.

\begin{figure*}
<<echo=FALSE, fig=TRUE, fig.height=3>>=
par(las=1)
plot(seq(-3,5,length=50),dnorm(seq(-3,5,length=50),1,1),type="l",xlab="",ylab="",ylim=c(0,0.5))
segments(x0 = c(-2,4),y0 = c(0,0),x1 = c(-2,4),y1=c(2,1))
text(x=1,y=0.45,cex=.8, labels = expression("99.7% of the data within 3" ~ s))
arrows(x0=c(-1,3),y0=c(0.45,0.45),x1=c(-2,4),y1=c(0.45,0.45))

segments(x0 = c(-1,3),y0 = c(0,0),x1 = c(-1,3),y1=c(0.4,0.4))
text(x=1,y=0.3, cex=.8, labels = expression("95% of the data within 2" ~ s))
arrows(x0=c(-0.5,2.5),y0=c(0.3,0.3),x1=c(-1,3),y1=c(0.3,0.3))

segments(x0 = c(0,2),y0 = c(0,0),x1 = c(0,2),y1=c(0.25,0.25))
text(x=1,y=0.15, cex=.8, labels = expression("68% of the data within 1" * s))
@
\caption{The variation of the population can be estimated with $s$, standard deviation.}
\label{fig:3rule}
\end{figure*}

When we calculate the sample mean we are usually interested not in the mean of this particular sample, but in the mean for individuals of this type---in statistical terms, of the population from which the sample comes. We usually collect data in order to generalise from them and so use the sample mean as an estimate of the mean for the whole population. Now the sample mean will vary from sample to sample; the way this variation occurs is described by the ``sampling distribution'' of the mean. We can estimate how much sample means will vary from the standard deviation of this sampling distribution, which we call the standard error (SE) of the estimate of the mean. As the standard error is a type of standard deviation, confusion is understandable. Another way of considering the \textbf{standard error is as a measure of the precision of the sample mean}.

The standard error of the sample mean depends on both the standard deviation and the sample size, by the simple relation $SE = SD/\sqrt{\textrm{sample size}}$. The standard error falls as the sample size increases, as the extent of chance variation is reduced---this idea underlies the sample size calculation for a controlled trial, for example. By contrast the standard deviation will not tend to change as we increase the size of our sample.

\section{Selecting $\alpha$: The Level of Confidence}

An important to note that the selection of the interval depends on your decision as to what level of confidence you want. Since probability ranges from 0 to 1, the confidence intervals of the parameter can also vary within this range. However, we usually are trying to constrain the confidence interval to something narrow, for example, we usually specify confidence intervals of 0.90, 0.95, and 0.99. 


In the context of a probability density function, these levels correspond to percentages of the area of the normal density curve. For example, a 95\% confidence interval includes 95\% of the normal curve's area -- the probability of observing a value outside of this area is less than 0.05. So, following standards in statistics, we use $\alpha$ to signify the as the criterion, such that

\begin{equation}
\textrm{Confidence Interval \% } = 100 * (1-\alpha)
\end{equation}

Yet, this is still ambiguous. Because the normal curve is symmetric, half of the area is in the left tail of the curve, and the other half of the area is in the right tail of the curve. Thus, if we want to generate confidence intervals that cover both tails of the curve we need to split $\alpha$ for each side of curve. Thus for a for a 95\% confidence interval, the area in each tail is equal to 0.05/2 = 0.025.\sidenote{A figure here would be helpful!} 

\section{Estimating Confidence Intervals for Waste Audit -- Preview}

\subsection{Calculating Summary Statitics}

First, we'll create summary statistics by each type of material: 

<<>>=
Sorted.mean <- aggregate(Sorted$Percent, 
  by=list(Type = Sorted$Type,  Trash_Recycle = Sorted$Trash_Recycle),
  mean)
Sorted.sd <- aggregate(Sorted$Percent, 
  by=list(Type = Sorted$Type, Trash_Recycle = Sorted$Trash_Recycle), 
  sd)
Sorted.n <- aggregate(Sorted$Percent, 
  by=list(Type = Sorted$Type, Trash_Recycle = Sorted$Trash_Recycle),
  length)

@

Here's the output of one of these lines: 

<<>>=
Sorted.mean
@

Next we'll put change the name of the variables and merge the datasets:\sidenote{Rarely do you need to do all this, but these data are a bit complicated. The example in the next section will be much eaiser to follow, so you can let your eyes glaze over the details here.}
<<>>=
  
names(Sorted.sd)= c("Type", "Trash_Recycle", "sd"); head(Sorted.sd)
names(Sorted.n)= c("Type", "Trash_Recycle", "n"); head(Sorted.n)
Sorted.mean
Sorted.SEM = merge(Sorted.sd, Sorted.n)
Sorted.Confidence = merge(Sorted.mean, Sorted.SEM)
Sorted.Confidence$SEM = Sorted.Confidence$sd/sqrt(Sorted.Confidence$n)

Sorted.Confidence <- droplevels(Sorted.Confidence)
@

In the next section, we are going to use the t-distribution to estimate our confidence intervals.

\subsection{Calculating a Parametric Confidence Interval}

To explore our waste audit data, we will determine the 95\% confidence intervals for the mean for each mean. As usual there are dozens of way to accomplish this, but for now, let's start by getting a t-statistic based on a criteria $\alpha$ value of 0.05. Since we calculate CI for the lower and upper limit, we need to split the probability in half ($\alpha/2$) and determine the intervals for 0.025 and 0.975 of the probably distribution.  

We begin by using the t-Distribution, which is a specialized case of the normal distribution (standard normal distribution that is corrected for sample size with change in the degrees of freedom).\sidenote{A figure here would be useful!}. 

\begin{equation}
\bar{x} - t_{\alpha/2, n-1}(sd/\sqrt{n}) < \mu < \bar{x} + t_{\alpha/2, n-1}(sd/\sqrt{n}) 
\end{equation}

<<>>=
alpha = 0.05
degfree = 4 - 1
qt(alpha/2, degfree)
@ 

<<echo=F, results='hide', label=parametricCI >>=	
Sorted.Confidence$CI.low  <- Sorted.Confidence$x + 
  qt(alpha/2, degfree)*Sorted.Confidence$SEM; Sorted.Confidence$CI.low
Sorted.Confidence$CI.high <- Sorted.Confidence$x + 
  qt(1-alpha/2, degfree)*Sorted.Confidence$SEM; Sorted.Confidence$CI.high

xvalues = as.numeric(Sorted.Confidence$Type[Sorted.Confidence$Trash_Recycle=="R"])-0.05
x1 = xvalues + 0.15; x1
Ylim = c(min(Sorted.Confidence$CI.low, na.rm=T), max(Sorted.Confidence$CI.high, na.rm=T))
@

\begin{figure*}
\caption{\% material from unsorted recycling and trash mass (95\% Confidence Intevals).}
\label{fig:percent_ci}
<<fig=T, fig.height=5>>=
par(cex=1.3, cex.axis=1)
plot(xvalues, ylim=Ylim, xlim=c(0.5,7.5),ty="n", xaxt='none', 
    xlab="Type", las=1, ylab="% of Unsorted Mass")
axis(side=1, at=1:7, label=levels(Sorted.Confidence$Type), 
    cex.axis=.5)
points(xvalues, 
    Sorted.Confidence$x[Sorted.Confidence$Trash_Recycle=="R"], 
    col="darkgreen", pch=19)
points(x1, Sorted.Confidence$x[Sorted.Confidence$Trash_Recycle=="T"], 
    col="Red", pch=2)
with(Sorted.Confidence[Sorted.Confidence$Trash_Recycle=="R",], 
     arrows(xvalues, CI.low, xvalues, CI.high, length=0.05, 
            angle=90, code=3, lwd=2, col="darkgreen"))
with(Sorted.Confidence[Sorted.Confidence$Trash_Recycle=="T",], 
     arrows(x1, CI.low, x1, CI.high, length=0.05, 
            angle=90, code=3, lwd=2, col="red"))
# Add a legend
legend(2, 95, legend=c("Recycling", "Trash"),
       col=c("darkgreen", "red"), pch=c(19, 2), cex=.8, bty='n')
@	
\end{figure*}


\section{Implementing Code to Create Confidence Intervals}

\subsection{Steps to Create Confidence Intervals}

This example assumes that the samples are drawn from a Normal distribution. The basic procedure for calculating a confidence interval for a population mean is as follows:
\begin{enumerate}
  \item Identify the sample mean, $\bar{x}$.
  
  \item Identify whether the population standard deviation is known, $\sigma$, or is unknown and is estimated by the sample standard deviation $s$.
  
  \begin{itemize}
    \item If the population standard deviation is known then $z^{*}=\Phi ^{-1}\left(1-{\frac {\alpha }{2}}\right)=-\Phi ^{-1}\left({\frac {\alpha }{2}}\right)$, where $C=100(1-\alpha)$ is the confidence level and $\Phi$  is the CDF of the standard normal distribution, used as the critical value. This value is only dependent on the confidence level for the test. Typical two sided confidence levels are:
    
\begin{table}
\begin{tabular}{ll}
C	    & z* \\ \hline
99\%	  & 2.576 \\
98\%	  & 2.326 \\
95\%	  & 1.96 \\
90\%	  & 1.645 \\
\end{tabular}
\end{table}

\item If the population standard deviation is unknown then the Student's t distribution is used as the critical value. This value is dependent on the confidence level ($C$) for the test and degrees of freedom. The degrees of freedom are found by subtracting one from the number of observations, $n − 1$. The critical value is found from the t-distribution table. In this table the critical value is written as $t^{*}=t_{\alpha }(r)$, where $r$ is the degrees of freedom and $\alpha ={1-C \over 2}$.

  \end{itemize}

\item Plug the found values into the appropriate equations:

\begin{itemize}
  \item For a known standard deviation: $\left({\bar {x}}-z^{*}{\sigma \over {\sqrt {n}}},{\bar {x}}+z^{*}{\sigma \over {\sqrt {n}}}\right)$

  \item For an unknown standard deviation: $\displaystyle \left({\bar {x}}-t^{*}{s \over {\sqrt {n}}},{\bar {x}}+t^{*}{s \over {\sqrt {n}}}\right)$
\end{itemize}

\end{enumerate}


\subsection{Normal versus the t-Distribution}

For demonstration purposes, we should compare the standard normal distribution to the t-distribuition so we understand what the implications of distribution might mean in hypothesis testing!

\begin{figure}
<<fig=TRUE>>=
xrange = seq(-4, 4, by=.02)
plot(xrange, dnorm(xrange, 0, sd=1), ty='l', ylim=c(0,.4), xlim=c(-4,4))

lines(xrange, dt(xrange, df=1), ty='l', col='red')
lines(xrange, dt(xrange, df=3), ty='l', col='orange')
lines(xrange, dt(xrange, df=10), ty='l', col='blue')
@
\end{figure}

Notice that the t-distribution is squatter in the middle and fatter at the edges. This means there is more probability ``area'' at the ends. But as you approach the n=20, the curves are increasingly overlapping.

\subsection{How to make these plots}

Notice that I used \texttt{dnorm()} and \texttt{dt()} functions to create the figures. What are these? These functions create probability density curves. It's cool to see that that there are several variants of these functions that are related. 

\begin{description}
  \item[\texttt{rnorm()}] generates n random numbers from a normal distribution with a defined mean and sd (standard deviation).
  \item[\texttt{dnorm()}] generates probability densities based on the quantile, mean, and sd. This can be based on an upper or lower side of the distribution. More on this below.
  \item[\texttt{qnorm()}] generates quantiles based on the probabilities, mean and sd. This can also be based on the upper or lower side of the distribution. 
  \item[\texttt{pnorm()}] generates probabilities based on the quantiles, mean and sd.
\end{description}

\noindent To test demonstrate these functions, let's try each out. First, I'll create a set of twenty random numbers (let's use the standard normal, which as a mean of zero and a sd of one) and round them to the nearest 100th. 

<<r randomnumbers>>= 
(randomnumbers = round(rnorm(20, mean=0, sd=1), 2))
#hist(randomnumbers)
@

Now, let's uses these figure out what the probabilies, using the default for lower tail, i.e. \texttt{lower.tail=T}. In other words, what is the probability that the number will be higher than the lower side of the tail. 

<<>>=
round(pnorm(randomnumbers, mean=0, sd=1, lower.tail=T), 2)
@

finally, let's see what the qauntiles look like based on various probabilities

<<>>=
round(qnorm(c(0.01, 0.025, .05, .1, .5, .9, .95, .975, 0.99), 
            mean=0, sd=1, lower.tail=T), 2)
@

If we switch these numbers to the 68-95-99.7 rule probabilies. First, we need to divide the tails in 1/2 and then we can put them in the \texttt{qnorm()} function. We can see where the the 1, 2 and 3 standard deviations came from (rounded right?). Pretty Neat!

<<>>=
percent.68 = (1 - 0.68)/2
percent.95 <- (1 - 0.95)/2 
percent.99 <- (1 - 0.997)/2
round(-qnorm(c(percent.68, percent.95, percent.99), mean=0, sd=1), 2)
@

Okay, let's plot these functions and see how they work!

<<>>=
par(mfrow=c(2,2))
plot(randomnumbers); hist(randomnumbers)
plot(randomnumbers, pnorm(randomnumbers, mean=0, sd=1), main="pnorm()")
plot(randomnumbers, dnorm(randomnumbers, mean=0, sd=1), main="dnorm()")

@

\subsection{Calculating Confidence Intervals with known $\sigma^2$.}

Because we have the values for the entire population, we can estimate confidence intervals using the normal distribution (z* scores) using the formula above. 

We'll create confidence intervals using our $n$=100 sample. 

The $\bar{x}$ = \Sexpr{round(mean(n100), 2)} and $\sigma$ = \Sexpr{sd(N1000)}. First, we'll create a function to calculate our confidence intervals that has the following inputs: sample vector, level of confidence ($\alpha$), and $\mu$.

<<>>=
cl<- function(sample, alpha, sigma){
  sem <- sigma/sqrt(length(sample))
  ucl <- mean(sample) + qnorm(1-alpha/2)*sem  
  lcl <- mean(sample) - qnorm(1-alpha/2)*sem  
  ci <- c(lcl, ucl)
  return(ci)
}

(cl95.n100 <- cl(n100, 0.05, sd(N1000)))
xvalues = seq(6,14,.1)
plot(xvalues, dnorm(xvalues, mean(n100), sd(n100)), ty='l')
abline(v=mean(n100))
abline(v=cl95.n100[1], col='grey')
abline(v=cl95.n100[2], col='grey')
@

\section{How does n influence confidence intervals?}

<<>>=
(cl95.n20 <- cl(n20, 0.05, sd(N1000)))
(cl95.n20 <- cl(n20, 0.05, sd(N1000)))
(cl95.n5 <- cl(n5, 0.05, sd(N1000)))

xvalues = seq(6,14,.1)
plot(xvalues, dnorm(xvalues, mean(n100), sd(n100)), ty='l', col='green', ylim=c(0, 0.6))
abline(v=mean(n100), col='green')
abline(v=cl95.n100[1], col='green')
abline(v=cl95.n100[2], col='green')

lines(xvalues, dnorm(xvalues, mean(n20), sd(n20)), ty='l', col='blue')
abline(v=cl95.n20[1], col='blue')
abline(v=cl95.n20[2], col='blue')

lines(xvalues, dnorm(xvalues, mean(n5), sd(n5)), ty='l', col='red')
abline(v=cl95.n5[1], col='red')
abline(v=cl95.n5[2], col='red')
@

\subsection{Confidence Intervals with an unknown $\sigma$}

<<>>=
qt((1-.95)/2, 4)*sd(n5)/sqrt(5)+mean(n5)
@
@

\subsection{Misunderstandings about Confidence Intervals}

Confidence intervals and levels are frequently misunderstood, and published studies have shown that even professional scientists often misinterpret them. I find myself tripped up the the vocabularly regularly. 

A 95\% confidence level does not mean that for a given realized interval there is a 95\% probability that the population parameter lies within the interval (i.e., a 95\% probability that the interval covers the population parameter). According to the strict frequentist interpretation, once an interval is calculated, this interval either covers the parameter value or it does not; it is no longer a matter of probability. 

The 95\% probability relates to \textbf{the reliability of the estimation procedure}, not to a specific calculated interval.

Neyman himself (the original proponent of confidence intervals) made this point in his original paper:

\begin{quote}
It will be noticed that in the above description, the probability statements refer to the problems of estimation with which the statistician will be concerned in the future. In fact, I have repeatedly stated that the frequency of correct results will tend to $\alpha$. Consider now the case when a sample is already drawn, and the calculations have given [particular limits]. Can we say that in this particular case the probability of the true value [falling between these limits] is equal to $\alpha$? The answer is obviously in the negative. The parameter is an unknown constant, and no probability statement concerning its value may be made \ldots
\end{quote}

\noindent Deborah Mayo expands on this further as follows:

\begin{quote}
It must be stressed, however, that having seen the value [of the data], Neyman-Pearson theory never permits one to conclude that the specific confidence interval formed covers the true value of 0 with either $(1 - \alpha)100\%$ probability or $(1 - \alpha)100\%$ degree of confidence. Seidenfeld's remark seems rooted in a (not uncommon) desire for Neyman-Pearson confidence intervals to provide something which they cannot legitimately provide; namely, a measure of the degree of probability, belief, or support that an unknown parameter value lies in a specific interval. Following Savage (1962), the probability that a parameter lies in a specific interval may be referred to as a measure of final precision. While a measure of final precision may seem desirable, and while confidence levels are often (wrongly) interpreted as providing such a measure, no such interpretation is warranted. Admittedly, such a misinterpretation is encouraged by the word 'confidence.'
\end{quote}

\begin{itemize}
  \item A 95\% confidence level does not mean that 95\% of the sample data lie within the confidence interval.
  \item A confidence interval is not a definitive range of plausible values for the sample parameter, though it may be understood as an estimate of plausible values for the population parameter.
  \item A particular confidence level of 95\% calculated from an experiment does not mean that there is a 95\% probability of a sample parameter from a repeat of the experiment falling within this interval.
\end{itemize}

\subsection{What what the heck is a confidence interval?}

Various interpretations of a confidence interval can be given (taking the 90\% confidence interval as an example in the following).

\begin{itemize}
  \item The confidence interval can be expressed in terms of samples (or repeated samples): ``Were this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 90\%.''

  \item The confidence interval can be expressed in terms of a single sample: ``There is a 90\% probability that the calculated confidence interval from some future experiment encompasses the true value of the population parameter.'' Note this is a probability statement about the confidence interval, not the population parameter. This considers the probability associated with a confidence interval from a pre-experiment point of view, in the same context in which arguments for the random allocation of treatments to study items are made. Here the experimenter sets out the way in which they intend to calculate a confidence interval and to know, before they do the actual experiment, that the interval they will end up calculating has a particular chance of covering the true but unknown value. This is very similar to the ``repeated sample'' interpretation above, except that it avoids relying on considering hypothetical repeats of a sampling procedure that may not be repeatable in any meaningful sense.

  \item The explanation of a confidence interval can amount to something like: ``The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not statistically significant at the 10\% level.'' In fact, this relates to one particular way in which a confidence interval may be constructed.

\end{itemize}

In each of the above, the following applies: If the true value of the parameter lies outside the 90\% confidence interval, then a sampling event has occurred which had a probability of 10\% (or less) of happening by chance.



\clearpage
\section{t-test}

\subsection{What is a (statisical) hypothesis?}

A hypothesis is a prediction --- and using the frequentists approach, we make a negative prediction, i.e. that there is no pattern, to test. In our example with the waste, we can test was the the amount recycling stastically significant compared to the total waste. In other words, were students diverting waste. We can test if the confidence intervals include 100\%. 

First, we'll subset the data!
<<>>=
trash = subset(Sorted, Type == "Trash", 
        select = c(Trash_Recycle, Type, Percent)); trash
@

<<>>=
t.test(trash[1:4,3], mu=100)
@

The t.test output is a bit confusioning. But let's start with the null hypothesis, which is the mean is equal to 100. The alternative is ``true mean is not equal to 100.'' Next, let's check out the p-value: \Sexpr{round(t.test(trash[1:4,3], mu=100)$p.value, 4)}.\sidenote{What should you report?  We don't need all these decimals!  Thus, we usually report p values as < 0.05, <0.01, and <0.001. It's a good practices and sufficiently informative for most purposes.}

Thus, we can reject the null hypothesis, the mean is equal to 100\%. Thus, we are testing if there is a difference between the total unsorted weight of the recyling and the weight of the trash in the recycling bins. We hope there is a difference! And there is because we can reject the null hypothesis. 

Thus, evidence students are diverting not just dumping stuff randomly in the recycling bins. I guess that is good news!

A second question is what there a signficant amount of recyclable material that should have been recycled?  What the amount of potentially diverted material statistically significant?

<<>>=
t.test(trash[5:8,3], mu=100)
@

This is not surprising, but disappointing. The amount of recycling in the trash is significantly below 100\%, that suggests recyclelable materials are being put into the trash in a signficant fashion. We might notice that one observation \# is a big problem. What is \#40?

<<>>=
Sorted[35,] # Not sure why this isn't #40, still sorting that out... !!!
@

No surprise -- the parties is Dialynas lead to terrible recycling behavior. However, with the new rules from WMI, perhaps they were anticipating the change and are actually better performers than we think! 

\subsection{Comparing Means}

We can then explore if the \% trash in recycling was different than the \% trash in the recycling. We test this with a null hypothesis, that there is no difference between the percentage.

<<>>=
trash.RT = subset(Sorted, Type == "Trash", 
            select = c(Trash_Recycle, Percent)); trash
t.test(trash.RT[1:4,2], trash.RT[5:8,2])
@


\begin{comment}

\section{Confidence Intervals and Expectations}

We should, however, be explicit in examining the nature of confidence interval within a framework of probability.  A confidence interval (CI) is population parameter estimate. Instead of estimating the parameter by a single value, an interval is likely to include the given parameter. A confidence interval is always qualified by a particular confidence level, usually expressed as a percentage; thus one speaks of a ''95\% confidence interval.'' The end points of the confidence interval are referred to as confidence limits.

<<echo = T, results = 'hide'>>=
TcCB_ref <- c(0.60, 0.50, 0.39, 0.84, 0.46, 0.39, 0.62, 0.67, 
0.69, 0.81, 0.38, 0.79, 0.43, 0.57, 0.74, 0.27, 0.51, 
0.35, 0.28, 0.45, 0.42, 1.14, 0.23, 0.72, 0.63, 0.50, 
0.29, 0.82, 0.54, 1.13, 0.56, 1.33, 0.56, 1.11, 0.57, 
0.89, 0.28, 1.20, 0.76, 0.26, 0.34, 0.52, 0.42, 0.22, 
0.33, 1.14, 0.48)
@


<<echo=F, results='hide'>>=
TcCB_cleanup <- c(NA, 0.09, 0.09, 0.12, 0.12, 0.14, 0.16, 
0.17, 0.17, 0.17, 0.18, 0.19, 0.20, 0.20, 
0.21, 0.21, 0.22, 0.22, 0.22, 0.23, 0.24, 
0.25, 0.25, 0.25, 0.25, 0.26, 0.28, 0.28, 
0.29, 0.31, 0.33, 0.33, 0.33, 0.34, 0.37,
0.38, 0.39, 0.40, 0.43, 0.43, 0.47, 0.48, 
0.48, 0.49, 0.51, 0.51, 0.54, 0.50, 0.61,
0.62, 0.75, 0.82, 0.85, 0.92, 0.94, 1.05,
1.10, 1.10, 1.19, 1.22, 1.33, 1.39, 1.39,
1.52, 1.53, 1.73, 2.35, 2.26, 2.59, 2.61,
3.06, 3.29,5.56, 6.61, 18.40, 51.97, 168.64) 

TcCB_ref_mean=mean(TcCB_ref)
TcCB_cleanup_mean = mean(TcCB_cleanup, na.rm=T)
@

When we develop a confidence 95\% confidence interval, we expect the \textbf{mean} to fall within the 95\% confidence intervals.  To illustrate, let's look at the copper data from the reference site we have looked at before, where the mean is \Sexpr{round(mean(TcCB_ref), 2)}. Thus, for a 95\% confidence interval, we expect the mean to be within the bounds 19 out of 20 times.


The calculation of a confidence interval generally requires assumptions about the nature of the estimation process. Within a parametric method, the confidence intervals depend on an assumption that the distribution of the population from which the sample came is theoretical distribution. If there no assumptions made about the distribution i.e. distribution free, then the confidence intervals are considered robust statistics. 

\subsection{Characteristics of Reliable Confidence Intervals}

Confidence intervals rely on three properties: validity, optimality, and invariance. When these properties are met, then the assumptions for the methods hold and the confidence intervals are thought to be more reliable. 

\begin{description}
	\item[Validity] means that the nominal coverage probability (confidence level) of the confidence interval should hold, either exactly or to a good approximation.
	\item[Optimality] means that the rule for constructing the confidence interval should make as much use of the information in the data-set as possible. For example, one could throw away half of a dataset and still be able to derive a valid confidence interval. One way of assessing optimality is by the length of the interval, so that a rule for constructing a confidence interval is judged better than another if it leads to intervals whose lengths are typically shorter.
	

\begin{marginfigure}
	\centering
		\includegraphics{Shoptalk_comic.jpg}
	\caption{Censored data make it difficult to ascertain parameter estimates -- so, more data high quality data is best. But if you have knowledge of low quality data, you might be able to justify removing it to create confidence intervals. But be careful!}
	\label{fig:Shoptalk_comic}
\end{marginfigure}

	\item[Invariance] In many applications the quantity being estimated might not be tightly defined as such. For example, a survey might result in an estimate of the median income in a population, but it might equally be considered as providing an estimate of the logarithm of the median income, given that this is a common scale for presenting graphical results. It would be desirable that the method used for constructing a confidence interval for the median income would give equivalent results when applied to constructing a confidence interval for the logarithm of the median income: Specifically the values at the ends of the latter interval would be the logarithms of the values at the ends of former interval.
\end{description}

\subsection{Calculating a Parametric Confidence Interval}

To explore our examples, we determine the 95\% confidence intervals for the mean of the copper data. As usual there are dozens of way to accomplish this, but for now, let's start by getting a t-statistic by our is that we use our $\alpha$ value of 0.05. Since we calculate CI for the lower and upper limit, we need to split the probability in half and determine the intervals for 0.025 and 0.975 of the probably distribution.  

We begin by using the t-Distribution, which is a specialized case of the normal distribution (standard normal distribution that is corrected for sample size with change in the degrees of freedom). 


\begin{equation}
\bar{x} - t_{\alpha/2, n-1}(s/\sqrt{n}) < \mu < \bar{x} + t_{\alpha/2, n-1}(s/\sqrt{n}) 
\end{equation}

<<>>=
alpha = 0.05
degfree = length(TcCB_ref) - 1
qt(alpha/2, degfree)
@ 

<<echo=F, results='hide' >>=	
CI.low  <- TcCB_ref_mean - qt(alpha/2, degfree)*sd(TcCB_ref)/sqrt(length(TcCB_ref)); CI.low
CI.high <- TcCB_ref_mean - qt(1-alpha/2, degfree)*sd(TcCB_ref)/sqrt(length(TcCB_ref)); CI.high

@	

Using the \texttt{qt()} in R function we find that confidence intervals are \Sexpr{round(CI.low, 2)} and \Sexpr{round(CI.high, 2)}. Remember, we have created confidence intervals where we expect the mean to fall 95\% of the time.

\begin{marginfigure}
\label{fig:ci}
\caption{95\% confidence intervals for the mean using the t-Distribution.}
<<echo = F, fig = T, label=parmetricfigure>>=
x=seq(-.5, 2 , by = .01)
y=dnorm(x, mean(TcCB_ref), sd(TcCB_ref))
plot(x,y, type="l", col="black", lwd=2)
abline(v=CI.low, col="blue", lwd=2)
abline(v=CI.high, col="red", lwd=2)
rug(TcCB_ref)
@

\end{marginfigure}

\section{Is the value outside the expected range?}

When our sample size is small relative to the population size, we can use the following formula to calculate a z-value, 

\begin{equation}
z = \frac{\bar{y}-\mu}{\sigma/\sqrt{n}}
\end{equation}

z follows a z-distribution or a standard normal distribution ($z\sim(0,1)$), thus can be used to evaluate probilities.  

With our ten numbers, we can then compare z to the standard normal distribution, 

<<>>=


@



\subsection{Distribution Free Confidence Intervals}

The simplest way to calculate a distribution-free confidence interval is to simply calculate the percentile of the data. However, rarely do we have enough data to have these percentages represented. So, what can we do?

In progress...

\subsection{Bootstrapping Confidence Intervals}

Similar to the distribution-free methods, the bootstrapping method creates confidence intervals based on the data themselves without the use of a theoretical distribution. In this case, we ''sample'' our dataset over and over again to create a distribution of t-values from which to create confidence intervals. 

Let's begin to sampling the copper data. First, we want to sample the data (with replacement) to create a sample. 

<<>>=
sample(TcCB_ref, length(TcCB_ref), replace=T)
@

We'll call it sample.bs (bs = bootstrapped)

<<>>=
sample.bs <- sample(TcCB_ref, length(TcCB_ref), replace=T)
@

From this, we can create a t-value using the following formula

\begin{equation}
t = (\bar{x} - \mu)/(s/sqrt{n})
\end{equation}

where $\mu$ is the population mean, \Sexpr{TcCB_ref_mean}, calculated from the original dataset. 

<< >>=
mean.bs <- mean(sample.bs); 
sd.bs <- sd(sample.bs); 
n.bs <- length(sample.bs)
t = (mean.bs - TcCB_ref_mean)/(sd.bs/sqrt(n.bs)); t
@

And now, we want to do this lots of time to create a distribution of $t_i$ that we can find the bottom 0.025\% and top 0.975\%. 

First, let's create a loop to iterate 20 times.\sidenote{Notice that I complicated the t calculation by putting in the mean, sd, and length functions within the formula.}

<<echo=T, results='hide', label=interations>>=
t <- vector()
iterations = 20
set.seed(666)
for(i in 1:iterations){
  sample.bs <- sample(x=TcCB_ref, size=length(TcCB_ref), replace=T)
  t[i] <- (mean(sample.bs) - TcCB_ref_mean)/(sd(sample.bs)/sqrt(length(sample.bs)))
}
@

Note that with 20 samples how ''bumpy'' the line looks (Figure \ref{fig:t20}). In fact, it appears to look nothing like the t-Distribution. Let's increase the number of iterations to 5000 and see what happens next.

\begin{marginfigure}
<<echo=F, fig=T, label=cdf_20>>=
y <- seq(0, 1, length.out = iterations)
par(las=1)
plot(sort(t), y, ty="l", lwd=2, ylab="Cumulative Probability", xlab="t-value")
lines(sort(t), pt(sort(t), degfree), col="green")
@
	\caption{t-values with 20 interactions}
	\label{fig:t20}
\end{marginfigure}


<<echo=F, results='hide'>>=
t <- vector()
iterations = 5000
set.seed(666)
for(i in 1:iterations){
  sample.bs <- sample(x=TcCB_ref, size=length(TcCB_ref), replace=T)
  t[i] <- (mean(sample.bs) - TcCB_ref_mean)/(sd(sample.bs)/sqrt(length(sample.bs)))
}
@


So, let's look at a histogram of the t-values we bootstrapped (Table \ref{fig:hist-bs}). Notice that the distribution approximates a normal distribution. In spite of this, the confidence intervals using this method will vary.


\begin{marginfigure}
<<echo=F, fig=T, label=histbs>>=
hist(t, main="", xlim=c(-5, 5))
@
	\caption{Histogram of 5,000 bootstrapped t-values.}
	\label{fig:hist-bs}
\end{marginfigure}

Now, let's create a cumulative distribution of our t-values. First, we put them in order and create a sequence of probabilities for each t-value (each one is equally probable, so this is pretty easy. We create a sequence of probabilities, y, that is the same number of interactions.

<<>>=
t <- sort(t)
y <- seq(0, 1, length.out = iterations)
@


\begin{figure}
<<echo=F, fig=T, label=cdf_5000>>= 
par(las=1)
plot(t, y, ty="l", lwd=2, ylab="Cummulative Probability", xlab="t-value")
abline(h=0.025, col="red"); text( 0, .07,"0.025% of the cdf",col="red")
abline(h=0.975, col="blue"); text( -3, .93,"0.975% of the cdf",col="blue")
lines(sort(t), pt(sort(t), degfree), col="green"); text(2, .5, "t-Distribution cdf",col="green")
@
	\caption{Cumlative with CIs}
	\label{CumCI}
\end{figure}

So, finally, we can query the t-values themselves to find the 0.025\% and 0.975\% percentiles. 

<<>>=
t.low  <- t[0.025 * iterations]
t.high <- t[0.975 * iterations]
@

Then we plug these into the following formula to get the confidence intervals

\begin{equation}
\bar{x} - t_{high}(s/\sqrt{n}) < \mu < \bar{x} - t_{low}(s/\sqrt{n}) 
\end{equation}

<<echo=F, results='hide'>>=
CI.bs.high <- TcCB_ref_mean - t.low*sd(TcCB_ref)/sqrt(length(TcCB_ref)); CI.bs.high
CI.bs.low  <- TcCB_ref_mean - t.high*sd(TcCB_ref)/sqrt(length(TcCB_ref)); CI.bs.low
@

And I get \Sexpr{round(CI.bs.low, 3)} $ < \mu < $ \Sexpr{round(CI.bs.high, 3)}. 

% Calculate values for clean up site.

<<echo=F, results='hide', label='CleanupSite'>>=
alpha = 0.05
degfree = length(TcCB_cleanup) - 1
qt(alpha/2, degfree)

CI.low  <- TcCB_cleanup_mean + qt(alpha/2, degfree)*sd(TcCB_cleanup, na.rm=T)/sqrt(length(TcCB_cleanup)); CI.low
CI.high <- TcCB_cleanup_mean + qt(1-alpha/2, degfree)*sd(TcCB_cleanup, na.rm=T)/sqrt(length(TcCB_cleanup)); CI.high

t <- vector()
iterations = 5000
set.seed(666)
for(i in 1:iterations){
  sample.bs <- sample(x=TcCB_cleanup, size=length(TcCB_cleanup), replace=T)
  t[i] <- (mean(sample.bs, na.rm=T) - TcCB_cleanup_mean)/(sd(sample.bs, na.rm=T)/sqrt(length(sample.bs)))
}

t <- sort(t)
y <- seq(0, 1, length.out = iterations)

t.low  <- t[0.025 * iterations]
t.high <- t[0.975 * iterations]

CI.bs.high <- TcCB_cleanup_mean - t.low*sd(TcCB_cleanup, na.rm=T)/sqrt(length(TcCB_cleanup)); CI.bs.high
CI.bs.low  <- TcCB_cleanup_mean - t.high*sd(TcCB_cleanup, na.rm=T)/sqrt(length(TcCB_cleanup)); CI.bs.low
@

Your answer may vary if you did not set the seed the same as I did. These are fairly close to the parametric estimates. Because these data are fairly well behaved, we didn't see a big difference, but when you do the same analysis with the cleaned up site the results are quite different. For the parametric results, the confidence intervals are \Sexpr{round(CI.low, 3)} and \Sexpr{round(CI.high, 3)} and for the bootstrapped values, \Sexpr{round(CI.bs.low, 3)} and \Sexpr{round(CI.bs.high, 3)}. 




% Book example -- but I didn't get the same results, but Manly may not have extrapolated carefully?

<<echo=F, results='hide', label=bookexample>>=
chl<-c(95,39,27,12.9,34.8,14.9,157,5.1,10.6,96,7.2,
       130,4.7,138,24.8,50,12.7,7.4,8.6,94,3.9,5,129,86,64)
mu <- mean(chl); mu
sample.bs <- sample(x=chl, size=25, replace=T)
mean.bs <- mean(sample.bs); mean.bs
t = (mean.bs - mu)/(sd(sample.bs)/sqrt(length(sample.bs))); t

t <- vector()
interations = 5000
set.seed(666)
for(i in 1:interations){
  sample.bs <- sample(x=chl, size=25, replace=T)
  t[i] <- (mean(sample.bs) - mu)/(sd(sample.bs)/sqrt(length(sample.bs)))
}

hist(t)
t <- sort(t)
y <- seq(0, 1, length.out = interations)
par(las=1)
plot(t, y, ty="l", lwd=2, ylab="Cumulative Probability", xlab="t-value")
abline(h=0.025, col="red"); text( -6, .07,"0.025% of the cdf",col="red")
abline(h=0.975, col="blue"); text( -6, .93,"0.975% of the cdf",col="blue")

t.low  <- t[0.025*interations]
t.high <- t[0.975 * interations]

CI.high <- mu - t.low*sd(chl)/sqrt(length(chl)); CI.high
CI.low  <- mu - t.high*sd(chl)/sqrt(length(chl)); CI.low
@



\subsection{Maximum Likelihood Methods}

To be developed!


\subsection{Bayesian Confidence Intervals}

To be developed!

Confidence intervals are one method of interval estimation, and the most widely used in frequentist statistics. An analogous concept in Bayesian statistics is credible intervals, while an alternative frequentist method is prediction interval, which, rather than estimating parameters, estimates the outcome of future samples.

There is disagreement about which of these methods produces the most useful results: the mathematics of the computations are rarely in question Ö confidence intervals being based on sampling distributions, credible intervals being based on Bayes' theorem Ö but the application of these methods, the utility and interpretation of the produced statistics, is debated.

Users of Bayesian methods, if they produced an interval estimate, would in contrast to confidence intervals, want to say "My degree of belief that the parameter is in fact in this interval is 90\%," while users of prediction intervals would instead say "I predict that the next sample will fall in this interval 90\% of the time."

Confidence intervals are an expression of probability and are subject to the normal laws of probability. If several statistics are presented with confidence intervals, each calculated separately on the assumption of independence, that assumption must be honored or the calculations will be rendered invalid. For example, if the statistic with the narrowest interval were selected for attention, that interval would no longer be the true interval for that statistic. The act of selection changes the probability and in this case widens the interval.

This is particularly important when confidence intervals are generated in order to perform statistical tests. If multiple tests are done and those that return positive results are selected from amongst them, the intervals used to conduct the test will change, and in most situations the tests will be rendered invalid.

A Bayesian interval estimate is called a credible interval. Using much of the same notation as above, the definition of a credible interval for the unknown true value of ? is, for a given ?[4],

  $  \Pr(u(x)<\Theta<v(x) | X = x)=1-\alpha. \, $

Here ? is used to emphasize that the unknown value of ? is being treated as a random variable. The definitions of the two types of intervals may be compared as follows.

    * The definition of a confidence interval involves probabilities calculated from the distribution of X for given (?, ?) (or conditional on these values) and the condition needs to hold for all values of (?, ?).
    * The definition of a credible interval involves probabilities calculated from the distribution of ? conditional on the observed values of X = x and marginalised (or averaged) over the values of ?, where this last quantity is the random variable corresponding to the uncertainty about the nuisance parameters in ?.

Note that the treatment of the nuisance parameters above is often omitted from discussions comparing confidence and credible intervals but it is markedly different between the two cases.

In some simple standard cases, the intervals produced as confidence and credible intervals from the same data set can be identical. They are always very different if moderate or strong prior information is included in the Bayesian analysis.



\section{Confidence Intervals Applied to Accuracy}

To be developed!

Accuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. This can be explicitly categorized in determine detection limits (Table \ref{tab:HypothesisTesting}).

\begin{table}
	\centering
		\begin{tabular}{l |l | l | l | l}
&&True Condition&& \\
&& True & False & \\
Test Outcome& Positive & True Positive & False Positive & Positive predictive value \\
Test Outcome& Negative & False Positive & True Negative & Negative predictive value \\
&& Sensitivity & Specificity & Accuracy	
		\end{tabular}
	\caption{Hypothesis Testing}
	\label{tab:HypothesisTesting}
\end{table}


That is, the accuracy is the proportion of true results (both true positives and true negatives) in the population. It is a parameter of the test.

\begin{equation}
accuracy=\frac{\text{number of true positives}+\text{number of true negatives}}{\text{numbers of true positives}+\text{false positives} + \text{false negatives} + \text{true negatives}}
\end{equation}

On the other hand, precision is defined as the proportion of the true positives against all the positive results (both true positives and false positives)

\begin{equation}
precision=\frac{\text{number of true positives}}{\text{number of true positives}+\text{false positives}}
\end{equation}

An accuracy of 100\% means that the measured values are exactly the same as the given values.

Also see Sensitivity and specificity.

Accuracy may be determined from Sensitivity and Specificity, provided Prevalence is known, using the equation:

    accuracy = (sensitivity)(prevalence) + (specificity)(1 - prevalence)

The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall.



\end{comment}

\FloatBarrier 
\begin{fullwidth}
% \renewcommand{\bibfont}{\small}
%\bibliography{LosHuertos_Complete_100420}
%\bibliographystyle{cbe}
\end{fullwidth}

\end{document}
